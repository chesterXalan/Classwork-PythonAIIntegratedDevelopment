{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eddbb46-d52b-4f70-9a1a-a11c7f69973c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18819189c9e44ba0bff7f4d6e0920875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d8342397f54762a528dee89a188aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2d3db3cc6943d1be2502f92d7df1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5cdbad89004a2e817c21428816f1cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120d09a734ab42a283c8758967c24a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743307611.945533   54775 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT 特徵維度: (3, 768)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "model_name = \"bert-base-chinese\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert_model = TFBertModel.from_pretrained(model_name)\n",
    "texts = [\"這是一個很棒的產品！\", \"這家餐廳的服務很差...\", \"電影的劇情令人感動！\"]\n",
    "inputs = tokenizer(\n",
    "    texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=512\n",
    ")\n",
    "outputs = bert_model(**inputs)\n",
    "cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "print(\"BERT 特徵維度:\", cls_embeddings.shape)\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input_layer = Input(shape=(768,))  # BERT 特徵維度\n",
    "x = Dense(128, activation=\"relu\")(input_layer)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "output_layer = Dense(2, activation=\"softmax\")(x)  # 二分類問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "585b2722-bc63-4be1-8dcf-95d0c754d8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1743307618.206774   55816 service.cc:152] XLA service 0x7f75d800e4a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1743307618.206818   55816 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n",
      "2025-03-30 12:06:58.229983: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1743307618.340942   55816 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/13\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 0.7696"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743307619.713507   55816 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - accuracy: 0.5436 - loss: 0.7395\n",
      "Epoch 2/5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6718 - loss: 0.6053 \n",
      "Epoch 3/5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7877 - loss: 0.5093 \n",
      "Epoch 4/5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8444 - loss: 0.4115 \n",
      "Epoch 5/5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9537 - loss: 0.3237 \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 806ms/step\n",
      "預測結果: [1 1 1]\n"
     ]
    }
   ],
   "source": [
    "classifier = Model(inputs=input_layer, outputs=output_layer)\n",
    "classifier.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "X_train = np.random.rand(100, 768)  # 100 筆訓練樣本，每個 768 維\n",
    "y_train = np.random.randint(0, 2, 100)  # 0（負面）或 1（正面）\n",
    "classifier.fit(X_train, y_train, epochs=5, batch_size=8)\n",
    "test_texts = [\"這部電影真的很棒！\", \"這間餐廳讓我失望透頂...\", \"這款手機的性能很不錯\"]\n",
    "test_inputs = tokenizer(\n",
    "    test_texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=512\n",
    ")\n",
    "test_outputs = bert_model(**test_inputs)\n",
    "test_cls_embeddings = test_outputs.last_hidden_state[:, 0, :]  # 取 CLS 向量\n",
    "predictions = classifier.predict(test_cls_embeddings)\n",
    "predicted_labels = predictions.argmax(axis=1)  # 取最大機率的類別\n",
    "print(\"預測結果:\", predicted_labels)\n",
    "# 假設 1 代表正向，0 代表負向"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c7e7e22-2512-4a8c-89c7-19020ba79567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 811ms/step\n",
      "預測結果: [1 1]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step\n",
      "預測結果: [1]\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\"這部電影不錯，但是有點無聊。\", \"這部電影有點無聊，但是不錯。\"]\n",
    "test_inputs = tokenizer(\n",
    "    test_texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=512\n",
    ")\n",
    "test_outputs = bert_model(**test_inputs)\n",
    "test_cls_embeddings = test_outputs.last_hidden_state[:, 0, :]\n",
    "predictions = classifier.predict(test_cls_embeddings)\n",
    "predicted_labels = predictions.argmax(axis=1)  # 取最大機率的類別\n",
    "print(\"預測結果:\", predicted_labels)\n",
    "test_texts = [\"這是一個非常非常非常棒的產品！\"]\n",
    "test_inputs = tokenizer(\n",
    "    test_texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=512\n",
    ")\n",
    "test_outputs = bert_model(**test_inputs)\n",
    "test_cls_embeddings = test_outputs.last_hidden_state[:, 0, :]\n",
    "predictions = classifier.predict(test_cls_embeddings)\n",
    "predicted_labels = predictions.argmax(axis=1)  # 取最大機率的類別\n",
    "print(\"預測結果:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb690a6-dfec-4828-b8ae-86139c786858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
