{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eddbb46-d52b-4f70-9a1a-a11c7f69973c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT 特徵維度: (3, 768)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "model_name = \"bert-base-chinese\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert_model = TFBertModel.from_pretrained(model_name)\n",
    "texts = [\"這是一個很棒的產品！\", \"這家餐廳的服務很差...\", \"電影的劇情令人感動！\"]\n",
    "inputs = tokenizer(\n",
    "    texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=512\n",
    ")\n",
    "outputs = bert_model(**inputs)\n",
    "cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "print(\"BERT 特徵維度:\", cls_embeddings.shape)\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input_layer = Input(shape=(768,))  # BERT 特徵維度\n",
    "x = Dense(128, activation=\"relu\")(input_layer)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "output_layer = Dense(2, activation=\"softmax\")(x)  # 二分類問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "585b2722-bc63-4be1-8dcf-95d0c754d8dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">98,432</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m98,432\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │           \u001b[38;5;34m8,256\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m130\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,818</span> (417.26 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m106,818\u001b[0m (417.26 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,818</span> (417.26 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m106,818\u001b[0m (417.26 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5337 - loss: 0.7177\n",
      "Epoch 2/5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6330 - loss: 0.6229\n",
      "Epoch 3/5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6192 - loss: 0.6338 \n",
      "Epoch 4/5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7096 - loss: 0.5469\n",
      "Epoch 5/5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7583 - loss: 0.4564 \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step\n",
      "預測結果: [0 1 0]\n"
     ]
    }
   ],
   "source": [
    "classifier = Model(inputs=input_layer, outputs=output_layer)\n",
    "classifier.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "classifier.summary()\n",
    "X_train = np.random.rand(100, 768)  # 100 筆訓練樣本，每個 768 維\n",
    "y_train = np.random.randint(0, 2, 100)  # 0（負面）或 1（正面）\n",
    "classifier.fit(X_train, y_train, epochs=5, batch_size=8)\n",
    "test_texts = [\"這部電影真的很棒！\", \"這間餐廳讓我失望透頂...\", \"這款手機的性能很不錯\"]\n",
    "test_inputs = tokenizer(\n",
    "    test_texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=512\n",
    ")\n",
    "test_outputs = bert_model(**test_inputs)\n",
    "test_cls_embeddings = test_outputs.last_hidden_state[:, 0, :]  # 取 CLS 向量\n",
    "predictions = classifier.predict(test_cls_embeddings)\n",
    "predicted_labels = predictions.argmax(axis=1)  # 取最大機率的類別\n",
    "print(\"預測結果:\", predicted_labels)\n",
    "# 假設 1 代表正向，0 代表負向"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c7e7e22-2512-4a8c-89c7-19020ba79567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "預測結果: [0 0]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "預測結果: [0]\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\"這部電影不錯，但是有點無聊。\", \"這部電影有點無聊，但是不錯。\"]\n",
    "test_inputs = tokenizer(\n",
    "    test_texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=512\n",
    ")\n",
    "test_outputs = bert_model(**test_inputs)\n",
    "test_cls_embeddings = test_outputs.last_hidden_state[:, 0, :]\n",
    "predictions = classifier.predict(test_cls_embeddings)\n",
    "predicted_labels = predictions.argmax(axis=1)  # 取最大機率的類別\n",
    "print(\"預測結果:\", predicted_labels)\n",
    "test_texts = [\"這是一個非常非常非常棒的產品！\"]\n",
    "test_inputs = tokenizer(\n",
    "    test_texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=512\n",
    ")\n",
    "test_outputs = bert_model(**test_inputs)\n",
    "test_cls_embeddings = test_outputs.last_hidden_state[:, 0, :]\n",
    "predictions = classifier.predict(test_cls_embeddings)\n",
    "predicted_labels = predictions.argmax(axis=1)  # 取最大機率的類別\n",
    "print(\"預測結果:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feb690a6-dfec-4828-b8ae-86139c786858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "預測結果: [0]\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\"這是一個非常非常非常棒的產品！\"]\n",
    "test_inputs = tokenizer(\n",
    "    test_texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=512\n",
    ")\n",
    "test_outputs = bert_model(**test_inputs)\n",
    "test_cls_embeddings = test_outputs.last_hidden_state[:, 0, :]\n",
    "predictions = classifier.predict(test_cls_embeddings)\n",
    "predicted_labels = predictions.argmax(axis=1)  # 取最大機率的類別\n",
    "print(\"預測結果:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db07e0a2-48d5-402d-9fda-19069fba60a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 258ms/step - accuracy: 0.4924 - loss: 0.6976\n",
      "Epoch 2/5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 247ms/step - accuracy: 0.4798 - loss: 0.6933\n",
      "Epoch 3/5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 263ms/step - accuracy: 0.4790 - loss: 0.6944\n",
      "Epoch 4/5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 252ms/step - accuracy: 0.5009 - loss: 0.6939\n",
      "Epoch 5/5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 251ms/step - accuracy: 0.5937 - loss: 0.6817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c26c06d160>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "input_layer = Input(shape=(768, 1))  # 修改\n",
    "x = LSTM(128, return_sequences=False)(input_layer)  # 修改  LSTM\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "output_layer = Dense(2, activation=\"softmax\")(x)\n",
    "lstm_classifier = Model(inputs=input_layer, outputs=output_layer)\n",
    "lstm_classifier.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "X_train_lstm = X_train.reshape(-1, 768, 1)\n",
    "lstm_classifier.fit(X_train_lstm, y_train, epochs=5, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fc10549-e310-4df2-a907-67e62c5d49d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350ms/step\n",
      "預測結果: [1 0 1]\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\"這部電影真的很棒！\", \"這間餐廳讓我失望透頂...\", \"這款手機的性能很不錯\"]\n",
    "test_inputs = tokenizer(\n",
    "    test_texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=512\n",
    ")\n",
    "test_outputs = bert_model(**test_inputs)\n",
    "test_cls_embeddings = test_outputs.last_hidden_state[:, 0, :]\n",
    "predictions = lstm_classifier.predict(test_cls_embeddings)\n",
    "predicted_labels = predictions.argmax(axis=1)  # 取最大機率的類別\n",
    "print(\"預測結果:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22af3d37-ca89-4873-8122-ceace93f50e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 459ms/step\n",
      "預測結果: [0 0]\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\"這部電影不錯，但是有點無聊。\", \"這部電影有點無聊，但是不錯。\"]\n",
    "test_inputs = tokenizer(\n",
    "    test_texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=512\n",
    ")\n",
    "test_outputs = bert_model(**test_inputs)\n",
    "test_cls_embeddings = test_outputs.last_hidden_state[:, 0, :]\n",
    "predictions = lstm_classifier.predict(test_cls_embeddings)\n",
    "predicted_labels = predictions.argmax(axis=1)  # 取最大機率的類別\n",
    "print(\"預測結果:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00366fb8-889b-4ef4-a7cc-5ea28347aff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
      "預測結果: [0]\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\"這是一個非常非常非常棒的產品！\"]\n",
    "test_inputs = tokenizer(\n",
    "    test_texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=512\n",
    ")\n",
    "test_outputs = bert_model(**test_inputs)\n",
    "test_cls_embeddings = test_outputs.last_hidden_state[:, 0, :]\n",
    "predictions = lstm_classifier.predict(test_cls_embeddings)\n",
    "predicted_labels = predictions.argmax(axis=1)  # 取最大機率的類別\n",
    "print(\"預測結果:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e5440c4-4c2f-488e-bd46-4539e4038678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 556ms/step - accuracy: 0.5523 - loss: 0.7897\n",
      "Epoch 2/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 559ms/step - accuracy: 0.5139 - loss: 0.6974\n",
      "Epoch 3/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 527ms/step - accuracy: 0.4577 - loss: 0.7375\n",
      "Epoch 4/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 547ms/step - accuracy: 0.6477 - loss: 0.6797\n",
      "Epoch 5/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 574ms/step - accuracy: 0.5545 - loss: 0.6964\n",
      "Epoch 6/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 513ms/step - accuracy: 0.5410 - loss: 0.6805\n",
      "Epoch 7/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 531ms/step - accuracy: 0.5890 - loss: 0.6664\n",
      "Epoch 8/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 512ms/step - accuracy: 0.6075 - loss: 0.6585\n",
      "Epoch 9/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 550ms/step - accuracy: 0.5971 - loss: 0.6571\n",
      "Epoch 10/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 555ms/step - accuracy: 0.5908 - loss: 0.6371\n",
      "Epoch 11/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 531ms/step - accuracy: 0.6671 - loss: 0.6343\n",
      "Epoch 12/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 546ms/step - accuracy: 0.8700 - loss: 0.5696\n",
      "Epoch 13/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 597ms/step - accuracy: 0.9532 - loss: 0.5255\n",
      "Epoch 14/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 582ms/step - accuracy: 0.7879 - loss: 0.4524\n",
      "Epoch 15/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 534ms/step - accuracy: 0.8635 - loss: 0.3902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x21a71937140>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "from transformers import (  # LSTM之後的修改? 使用 BERT 隱層輸出（整個序列）作為 LSTM 輸入 。\n",
    "    BertTokenizer,\n",
    "    TFBertModel,\n",
    ")\n",
    "\n",
    "model_name = \"bert-base-chinese\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert_model = TFBertModel.from_pretrained(model_name)\n",
    "texts = [\"這是一個很棒的產品！\", \"這家餐廳的服務很差...\", \"電影的劇情令人感動！\"]\n",
    "inputs = tokenizer(\n",
    "    texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=512\n",
    ")\n",
    "outputs = bert_model(**inputs)\n",
    "sequence_output = outputs.last_hidden_state  # (batch_size, seq_length, hidden_dim)\n",
    "input_layer = Input(shape=(512, 768))  # BERT 由50改為預設的512\n",
    "x = LSTM(128, return_sequences=True)(input_layer)\n",
    "x = LSTM(64)(x)\n",
    "output_layer = Dense(2, activation=\"softmax\")(x)  # 二分類\n",
    "lstm_model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "lstm_model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "X_train = np.random.rand(\n",
    "    100, 512, 768\n",
    ")  # 100 筆訓練樣本，每個 512 tokens，每個 token 768 維\n",
    "y_train = np.random.randint(0, 2, 100)\n",
    "lstm_model.fit(X_train, y_train, epochs=15, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19b9afb8-55b8-47d6-84e0-ed6c4f87e4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 15, 768)\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "Epoch 1/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 534ms/step - accuracy: 0.4635 - loss: 0.7107\n",
      "Epoch 2/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 543ms/step - accuracy: 0.4769 - loss: 0.6937\n",
      "Epoch 3/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 547ms/step - accuracy: 0.5572 - loss: 0.7090\n",
      "Epoch 4/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 559ms/step - accuracy: 0.5709 - loss: 0.7007\n",
      "Epoch 5/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 599ms/step - accuracy: 0.5592 - loss: 0.6754\n",
      "Epoch 6/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 622ms/step - accuracy: 0.6631 - loss: 0.6653\n",
      "Epoch 7/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 614ms/step - accuracy: 0.6746 - loss: 0.6521\n",
      "Epoch 8/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 594ms/step - accuracy: 0.5310 - loss: 0.6340\n",
      "Epoch 9/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 635ms/step - accuracy: 0.7707 - loss: 0.5853\n",
      "Epoch 10/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 634ms/step - accuracy: 0.8492 - loss: 0.5039\n",
      "Epoch 11/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 612ms/step - accuracy: 0.9694 - loss: 0.3890\n",
      "Epoch 12/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 590ms/step - accuracy: 0.9843 - loss: 0.2566\n",
      "Epoch 13/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 571ms/step - accuracy: 1.0000 - loss: 0.1447\n",
      "Epoch 14/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 605ms/step - accuracy: 1.0000 - loss: 0.0590\n",
      "Epoch 15/15\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 581ms/step - accuracy: 1.0000 - loss: 0.0303\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 469ms/step\n",
      "預測結果: [0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "test_texts = [\"這部電影真的很棒！\", \"這間餐廳讓我失望透頂...\", \"這款手機的性能很不錯\"]\n",
    "test_inputs = tokenizer(\n",
    "    test_texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=512\n",
    ")\n",
    "test_outputs = bert_model(**test_inputs)\n",
    "#\n",
    "# inputs = tokenizer(texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=512)\n",
    "# outputs = bert_model(**inputs)\n",
    "#\n",
    "# test_cls_embeddings = test_outputs.last_hidden_state[:, 0, :]\n",
    "test_cls_embeddings = test_outputs.last_hidden_state\n",
    "print(test_cls_embeddings.shape)\n",
    "print(type(test_inputs))\n",
    "print(test_inputs.keys())  # 確保包含 'input_ids' 和 'attention_mask'\n",
    "\n",
    "\n",
    "# LSTM 輸入層\n",
    "input_layer = Input(shape=(None, 768))  # `None` 允許變長輸入\n",
    "# input_layer = Input(shape=(512, 768))  # 512 為 BERT 序列長度, 768 為 BERT hidden size\n",
    "x = LSTM(128, return_sequences=True)(input_layer)\n",
    "x = LSTM(64)(x)\n",
    "output_layer = Dense(2, activation=\"softmax\")(x)  # 二分類\n",
    "\n",
    "lstm_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "lstm_model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# 假設訓練數據\n",
    "X_train = np.random.rand(\n",
    "    100, 512, 768\n",
    ")  # 100 筆訓練樣本，每個 512 tokens，每個 token 768 維\n",
    "y_train = np.random.randint(0, 2, 100)\n",
    "\n",
    "# 訓練 LSTM\n",
    "lstm_model.fit(X_train, y_train, epochs=15, batch_size=8)\n",
    "\n",
    "# 測試時轉換為 NumPy\n",
    "test_cls_embeddings_np = test_cls_embeddings.numpy()\n",
    "predictions = lstm_model.predict(test_cls_embeddings_np)\n",
    "predicted_labels = predictions.argmax(axis=1)  # 取最大機率的類別\n",
    "print(\"預測結果:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eabc8d10-ffd8-462d-a27d-6373f94989a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 545ms/step\n",
      "預測結果: [0 0]\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\"這部電影不錯，但是有點無聊。\", \"這部電影有點無聊，但是不錯。\"]\n",
    "test_inputs = tokenizer(\n",
    "    test_texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=512\n",
    ")\n",
    "test_outputs = bert_model(**test_inputs)\n",
    "test_cls_embeddings = test_outputs.last_hidden_state\n",
    "# 測試時轉換為 NumPy\n",
    "test_cls_embeddings_np = test_cls_embeddings.numpy()\n",
    "predictions = lstm_model.predict(test_cls_embeddings_np)\n",
    "predicted_labels = predictions.argmax(axis=1)  # 取最大機率的類別\n",
    "print(\"預測結果:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9649487b-0fba-47df-98df-6984980a6841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "預測結果: [0]\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\"這是一個非常非常非常棒的產品！\"]\n",
    "test_inputs = tokenizer(\n",
    "    test_texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=512\n",
    ")\n",
    "test_outputs = bert_model(**test_inputs)\n",
    "test_cls_embeddings = test_outputs.last_hidden_state\n",
    "# 測試時轉換為 NumPy\n",
    "test_cls_embeddings_np = test_cls_embeddings.numpy()\n",
    "predictions = lstm_model.predict(test_cls_embeddings_np)\n",
    "predicted_labels = predictions.argmax(axis=1)  # 取最大機率的類別\n",
    "print(\"預測結果:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3b4a464-f392-4379-a5d0-0b14fc301ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 10s/step - accuracy: 0.5216 - loss: 0.6932 \n",
      "Epoch 2/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 10s/step - accuracy: 0.4953 - loss: 0.6933 \n",
      "Epoch 3/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 10s/step - accuracy: 0.5077 - loss: 0.6932 \n",
      "Epoch 4/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 10s/step - accuracy: 0.4792 - loss: 0.6932 \n",
      "Epoch 5/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 11s/step - accuracy: 0.5024 - loss: 0.6932\n",
      "Epoch 6/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 11s/step - accuracy: 0.4863 - loss: 0.6932 \n",
      "Epoch 7/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 11s/step - accuracy: 0.4866 - loss: 0.6932 \n",
      "Epoch 8/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 11s/step - accuracy: 0.5156 - loss: 0.6931 \n",
      "Epoch 9/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 11s/step - accuracy: 0.5068 - loss: 0.6931 \n",
      "Epoch 10/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 11s/step - accuracy: 0.4987 - loss: 0.6932 \n",
      "Epoch 11/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 10s/step - accuracy: 0.5020 - loss: 0.6932 \n",
      "Epoch 12/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 11s/step - accuracy: 0.4984 - loss: 0.6932 \n",
      "Epoch 13/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 11s/step - accuracy: 0.5072 - loss: 0.6931 \n",
      "Epoch 14/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 12s/step - accuracy: 0.4650 - loss: 0.6932 \n",
      "Epoch 15/15\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 12s/step - accuracy: 0.4574 - loss: 0.6932 \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "預測結果: [1 1]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "預測結果: [1]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "預測結果: [1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Multiply\n",
    "from tensorflow.keras.models import Model\n",
    "from transformers import (  # LSTM之後的修改? 使用 BERT 隱層輸出（整個序列）作為 LSTM 輸入 。\n",
    "    BertTokenizer,\n",
    "    TFBertModel,\n",
    ")\n",
    "\n",
    "model_name = \"bert-base-chinese\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert_model = TFBertModel.from_pretrained(model_name)\n",
    "\n",
    "# 假設訓練數據\n",
    "X_train = np.random.rand(\n",
    "    1000, 512, 768\n",
    ")  # 100 筆訓練樣本，每個 512 tokens，每個 token 768 維\n",
    "y_train = np.random.randint(0, 2, 1000)\n",
    "\n",
    "\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(\n",
    "            shape=(input_shape[-1],), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        score = tf.nn.softmax(tf.tensordot(inputs, self.W, axes=1), axis=1)  # 計算權重\n",
    "        return Multiply()([inputs, tf.expand_dims(score, -1)])  # 權重應用到輸入\n",
    "\n",
    "\n",
    "input_layer = Input(shape=(None, 768))\n",
    "\n",
    "# x = LSTM(128, return_sequences=True)(input_layer)\n",
    "# x = LSTM(64)(x)\n",
    "\n",
    "x = LSTM(256, dropout=0.2, return_sequences=True)(input_layer)\n",
    "x = AttentionLayer()(x)  # 應用注意力機制\n",
    "x = LSTM(128, dropout=0.2, return_sequences=True)(x)\n",
    "x = AttentionLayer()(x)  # 應用注意力機制\n",
    "x = LSTM(64, dropout=0.2)(x)\n",
    "output_layer = Dense(2, activation=\"softmax\")(x)\n",
    "lstm_attention_model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "lstm_attention_model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "lstm_attention_model.fit(X_train, y_train, epochs=15, batch_size=128)\n",
    "\n",
    "test_texts = [\"這部電影不錯，但是有點無聊。\", \"這部電影有點無聊，但是不錯。\"]\n",
    "test_inputs = tokenizer(\n",
    "    test_texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=512\n",
    ")\n",
    "test_outputs = bert_model(**test_inputs)\n",
    "# test_cls_embeddings = test_outputs.last_hidden_state[:, 0, :]\n",
    "test_embeddings = test_outputs.last_hidden_state  # 保持形狀 (batch_size, 512, 768)\n",
    "\n",
    "predictions = lstm_attention_model.predict(test_embeddings)\n",
    "predicted_labels = predictions.argmax(axis=1)  # 取最大機率的類別\n",
    "print(\"預測結果:\", predicted_labels)\n",
    "\n",
    "test_texts = [\"這是一個非常非常非常棒的產品！\"]\n",
    "test_inputs = tokenizer(\n",
    "    test_texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=512\n",
    ")\n",
    "test_outputs = bert_model(**test_inputs)\n",
    "test_cls_embeddings = test_outputs.last_hidden_state\n",
    "# 測試時轉換為 NumPy\n",
    "test_cls_embeddings_np = test_cls_embeddings.numpy()\n",
    "predictions = lstm_attention_model.predict(test_cls_embeddings_np)\n",
    "predicted_labels = predictions.argmax(axis=1)  # 取最大機率的類別\n",
    "print(\"預測結果:\", predicted_labels)\n",
    "\n",
    "\n",
    "test_texts = [\"這部電影真的很棒！\", \"這間餐廳讓我失望透頂...\", \"這款手機的性能很不錯\"]\n",
    "test_inputs = tokenizer(\n",
    "    test_texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=512\n",
    ")\n",
    "test_outputs = bert_model(**test_inputs)\n",
    "test_cls_embeddings = test_outputs.last_hidden_state\n",
    "# 測試時轉換為 NumPy\n",
    "test_cls_embeddings_np = test_cls_embeddings.numpy()\n",
    "predictions = lstm_attention_model.predict(test_cls_embeddings_np)\n",
    "predicted_labels = predictions.argmax(axis=1)  # 取最大機率的類別\n",
    "print(\"預測結果:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f56b6df-d369-4099-a55f-4f5c4afc5e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
